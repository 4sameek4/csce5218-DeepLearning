{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
       " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
       " '',\n",
       " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
       " '100 11645  100 11645    0     0  84958      0 --:--:-- --:--:-- --:--:-- 86902']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!!curl --output test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/refs/heads/main/test_small.txt\n",
    "!!curl --output train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/refs/heads/main/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "\n",
    "# Displaying first 10 lines of train.dat\n",
    "with open(\"train.dat\", \"r\") as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1\tX2\tX3\n",
      "1\t1\t1\t1\n",
      "0\t0\t1\t1\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "0\t1\t1\t0\n",
      "1\t1\t1\t1\n"
     ]
    }
   ],
   "source": [
    "# Displaying first 10 lines of test.dat\n",
    "with open(\"test.dat\", \"r\") as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array 2\n",
    "\n",
    "    paired_val = zip(array1, array2)\n",
    "    products = []\n",
    "\n",
    "    for a,b in paired_val:\n",
    "        multiply = a * b\n",
    "        products.append(multiply)\n",
    "\n",
    "    result = sum(products)\n",
    "    \n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    sig_result = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    return sig_result\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model \n",
    "    dot_prod = dot_product(weight, instance)\n",
    "    out = sigmoid(dot_prod)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    dot_prod = dot_product(weights, instance)\n",
    "    model_out = sigmoid(dot_prod)\n",
    "\n",
    "    if model_out >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #TODO: name this step\n",
    "    # Answer - Initializing the weights\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            #TODO: name these steps\n",
    "            # Answer - Forward Pass\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            #TODO: name these steps\n",
    "            # Answer - Backpropagation\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "#### For the predict() function, the return value is either 0 or 1. But the sigmoid() function returns a floating value within continuous range of 0 to 1. In our case, we need the return value to be a continuous value within the range 0 to 1 which is needed for gradient descent to calculate the error and update the weights accordingly. So we need to use the sigmoid() function.\n",
    "#### Moreover the predict() function is mainly used to give a prediction value, whereas the sigmoid() function is used as an activation function here for the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": [
    "#### The conclusions that we can see from the output given in Question 2 are :\n",
    " - Inference from Training Instances :\n",
    "   > For some cases, using 200 or more tr with lr of 0.005 and 0.05 gave the highest accuracy of 85.7%.\n",
    "   > \n",
    "   > For some cases, increasing the tr without adjusting other hyperparameters still resulted in the accuracy of 71.4% or less and did not increase.\n",
    " - Inference from Epochs :\n",
    "   > In most of the cases, increasing the number of epoch does not really increase the accuracy more than 71.4%.\n",
    "   > \n",
    "   > Only in very few cases having higher epochs like 100 gives the highest accuracy of 85.7% when the learning rate is 0.005 and 200 training insrances are taken with it.\n",
    "   > \n",
    "   > In some cases, when the epoch is increased with learning rate of 0.01 and 0.05 and training instances of 100, it is reducing the accuracy very much to 28.6%.\n",
    " - Inference from Learning Rate :\n",
    "   > When the learning rate is low (0.005) it takes too many epochs to get accuracy max up to 71.4% and sometimes upto 85.7%.\n",
    "   > \n",
    "   > When learning rate is high, the training becomes unstable and in multiple cases, it drops the accuracy to even 28.6%\n",
    "   > \n",
    "   > When the learning is moderate, it is able to reach the highest accuracy of 85.7% but again drops the accuracy when the epochs are set too high for this learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer A.\n",
    "No. We do not need to train with the whole dataset to reach the highest accuracy.\n",
    "\n",
    "> One such example : #tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
    "\n",
    "Here we are only using 20 training instances and getting the highest accuracy.\n",
    "\n",
    "This shows that a well chosen subset of the training datasert can perform as good as the whole dataset or even better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer B.\n",
    "At any point of running the code if the accuracy drops low when it uses more training data as compared to a scenario where it gives better accuracy with lesser training data, it is mainly because of the overfitting. It might also be an inference that most of the data is not informative or noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer C.\n",
    "Yes. The accuracy can be increased in the following ways.\n",
    "> Firstly, like we can use Adam or RMSProp optimizer instead of SGD.\n",
    "> \n",
    "> Secondly, we can use a L2 Regularization to prevent overly large weights.\n",
    ">\n",
    "> Thirdly, we can use dropout to randomly drop neurons to get better generalization of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer D.\n",
    "No. It is not always necessary to train for more epochs. Sometimes, higher number of epochs can even reduce the accuracy due to overfitting.\n",
    "So we use a mechanism to stop the epochs at an early stage than what we set initially, when the error does not change much. This mechanism is called early stopping and we set a patience which is the number of epochs still to run even if the error is not changing much. This is mainly used to overcome a saddle point or a local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
